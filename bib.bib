@incollection{baconBaconNewOrganon,
  title = {Bacon, {{New Organon I}}, {{Aphorisms}} 1-3, 11-31, and 36-46},
  booktitle = {Modern {{Philosophy}}: An Anthology of Primary Sources},
  author = {Bacon, Fransis},
  translator = {Ariew, Roger and Watkins, Eric},
  edition = {3},
  pages = {16--20},
  publisher = {Hackett Publisching Company, Inc.},
  location = {Indianapolis}
}

@article{denningComputerScienceScience2005,
  title = {Is Computer Science Science?},
  author = {Denning, Peter},
  date = {2005-04-01},
  journaltitle = {Commun. ACM},
  shortjournal = {Commun. ACM},
  volume = {48},
  number = {4},
  pages = {27--31}
}

@inproceedings{krebsbachComputerScienceNot2015,
  title = {Computer {{Science}}: {{Not}} about {{Computers}}, {{Not Science}}},
  author = {Krebsbach, Kurt D.},
  date = {2015},
  publisher = {Lawrence University},
  location = {Appleton}
}

@article{leisersonTherePlentyRoom2020,
  title = {There’s Plenty of Room at the {{Top}}: {{What}} Will Drive Computer Performance after {{Moore}}’s Law?},
  author = {Leiserson, Charles E. and Thompson, Neil C. and Emer, Joel S. and Kuszmaul, Bradley C. and Lampson, Butler W. and Sanchez, Daniel and Schardl, Tao B.},
  date = {2020},
  volume = {368},
  number = {6495},
  location = {Washington},
  issn = {0036-8075},
  abstract = {From bottom to top The doubling of the number of transistors on a chip every 2 years, a seemly inevitable trend that has been called Moore's law, has contributed immensely to improvements in computer performance. However, silicon-based transistors cannot get much smaller than they are today, and other approaches should be explored to keep performance growing. Leiserson et al. review recent examples and argue that the most promising place to look is at the top of the computing stack, where improvements in software, algorithms, and hardware architecture can bring the much-needed boost. Science , this issue p. eaam9744 BACKGROUND Improvements in computing power can claim a large share of the credit for many of the things that we take for granted in our modern lives: cellphones that are more powerful than room-sized computers from 25 years ago, internet access for nearly half the world, and drug discoveries enabled by powerful supercomputers. Society has come to rely on computers whose performance increases exponentially over time. Much of the improvement in computer performance comes from decades of miniaturization of computer components, a trend that was foreseen by the Nobel Prize–winning physicist Richard Feynman in his 1959 address, “There’s Plenty of Room at the Bottom,” to the American Physical Society. In 1975, Intel founder Gordon Moore predicted the regularity of this miniaturization trend, now called Moore’s law, which, until recently, doubled the number of transistors on computer chips every 2 years. Unfortunately, semiconductor miniaturization is running out of steam as a viable way to grow computer performance—there isn’t much more room at the “Bottom.” If growth in computing power stalls, practically all industries will face challenges to their productivity. Nevertheless, opportunities for growth in computing performance will still be available, especially at the “Top” of the computing-technology stack: software, algorithms, and hardware architecture. ADVANCES Software can be made more efficient by performance engineering: restructuring software to make it run faster. Performance engineering can remove inefficiencies in programs, known as software bloat, arising from traditional software-development strategies that aim to minimize an application’s development time rather than the time it takes to run. Performance engineering can also tailor software to the hardware on which it runs, for example, to take advantage of parallel processors and vector units. Algorithms offer more-efficient ways to solve problems. Indeed, since the late 1970s, the time to solve the maximum-flow problem improved nearly as much from algorithmic advances as from hardware speedups. But progress on a given algorithmic problem occurs unevenly and sporadically and must ultimately face diminishing returns. As such, we see the biggest benefits coming from algorithms for new problem domains (e.g., machine learning) and from developing new theoretical machine models that better reflect emerging hardware. Hardware architectures can be streamlined—for instance, through processor simplification, where a complex processing core is replaced with a simpler core that requires fewer transistors. The freed-up transistor budget can then be redeployed in other ways—for example, by increasing the number of processor cores running in parallel, which can lead to large efficiency gains for problems that can exploit parallelism. Another form of streamlining is domain specialization, where hardware is customized for a particular application domain. This type of specialization jettisons processor functionality that is not needed for the domain. It can also allow more customization to the specific characteristics of the domain, for instance, by decreasing floating-point precision for machine-learning applications. In the post-Moore era, performance improvements from software, algorithms, and hardware architecture will increasingly require concurrent changes across other levels of the stack. These changes will be easier to implement, from engineering-management and economic points of view, if they occur within big system components: reusable software with typically more than a million lines of code or hardware of comparable complexity. When a single organization or company controls a big component, modularity can be more easily reengineered to obtain performance gains. Moreover, costs and benefits can be pooled so that important but costly changes in one part of the big component can be justified by benefits elsewhere in the same component. OUTLOOK As miniaturization wanes, the silicon-fabrication improvements at the Bottom will no longer provide the predictable, broad-based gains in computer performance that society has enjoyed for more than 50 years. Software performance engineering, development of algorithms, and hardware streamlining at the Top can continue to make computer applications faster in the post-Moore era. Unlike the historical gains at the Bottom, however, gains at the Top will be opportunistic, uneven, and sporadic. Moreover, they will be subject to diminishing returns as specific computations become better explored. Performance gains after Moore’s law ends. In the post-Moore era, improvements in computing power will increasingly come from technologies at the “Top” of the computing stack, not from those at the “Bottom”, reversing the historical trend. CREDIT: N. CARY/ SCIENCE The miniaturization of semiconductor transistors has driven the growth in computer performance for more than 50 years. As miniaturization approaches its limits, bringing an end to Moore’s law, performance gains will need to come from software, algorithms, and hardware. We refer to these technologies as the “Top” of the computing stack to distinguish them from the traditional technologies at the “Bottom”: semiconductor physics and silicon-fabrication technology. In the post-Moore era, the Top will provide substantial performance gains, but these gains will be opportunistic, uneven, and sporadic, and they will suffer from the law of diminishing returns. Big system components offer a promising context for tackling the challenges of working at the Top.},
  keywords = {Algorithms,Bloat,Company structure,Complexity,Computation,Computer applications,Computer architecture,Computer components,Computer Oriented Programs,Computer programs,Computer simulation,Computer Software,Cost benefit analysis,Domains,Engineering,Fabrication,Floating point arithmetic,Hardware,Integrated circuits,Learning algorithms,Machine learning,Manufacturing,Microprocessors,Miniaturization,Modularity,Moore's law,Performance engineering,Processing speed,Reengineering,Reusable components,Semiconductor devices,Silicon,Software,Software development,Software engineering,Specialization,Steam,Streamlining,Supercomputers,Technology,Transistors}
}

@article{schallerMooreLawPresent1997,
  title = {Moore's Law: Past, Present and Future},
  author = {Schaller, R.R.},
  date = {1997},
  journaltitle = {SPEC},
  shortjournal = {SPEC},
  volume = {34},
  number = {6},
  pages = {52--59},
  location = {New York, NY},
  issn = {0018-9235},
  abstract = {A simple observation, made over 30 years ago, on the growth in the number of devices per silicon die has become the central driving force of one of the most dynamic of the world's industries. Because of the accuracy with which Moore's Law has predicted past growth in IC complexity, it is viewed as a reliable method of calculating future trends as well, setting the pace of innovation, and defining the rules and the very nature of competition. And since the semiconductor portion of electronic consumer products keeps growing by leaps and bounds, the Law has aroused in users and consumers an expectation of a continuous stream of faster, better, and cheaper high-technology products. Even the policy implications of Moore's Law are significant: it is used as the baseline assumption in the industry's strategic road map for the next decade and a half.},
  keywords = {Applied sciences,Consumer products,Electronics,Exact sciences and technology,General (including economical and industrial fields),Integrated circuits,Inventions,Inventors,Moore's Law,Roads,Semiconductors,Silicon,Technological innovation,Theory}
}

@article{waldropChipsAreMoore2016,
  title = {The Chips Are down for {{Moore}}’s Law},
  author = {Waldrop, M. Mitchell},
  date = {2016},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {530},
  number = {7589},
  pages = {144--147},
  location = {London},
  issn = {0028-0836},
  abstract = {The semiconductor industry will soon abandon its pursuit of Moore's law. Now things could get a lot more interesting.},
  keywords = {639/166,639/705/117,639/705/258,706/703/559,Humanities and Social Sciences,multidisciplinary,news-feature,Science}
}

@article{wirthPleaLeanSoftware1995,
  title = {A Plea for Lean Software},
  author = {Wirth, N.},
  date = {1995},
  journaltitle = {Computer},
  shortjournal = {MC},
  volume = {28},
  number = {2},
  pages = {64--68},
  location = {New York},
  issn = {0018-9162},
  abstract = {Software's girth has surpassed its functionality, largely because hardware advances make this possible. The way to streamline software lies in disciplined methodologies and a return to the essentials. The paper discusses some causes of "fat software" and considers the Oberon system whose primary goal was to show that software can be developed with a fraction of the memory capacity and processor power usually required, without sacrificing flexibility, functionality, or user convenience.{$<$} {$>$}},
  keywords = {Computer displays,Computer programming,Explosions,Hardware,Memory management,Operating systems,Program processors,Software,Software performance,Solids,Space technology,Workstations}
}
